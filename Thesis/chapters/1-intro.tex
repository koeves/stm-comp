\epigraph{"He who controls the spice controls the universe."}{---Frank Herbert, \textit{Dune}}

\section{Background}
In the era of multi- and many-core systems, achieving parallelisation to a sufficiently high degree is of great importance. Speeding up sequential applications by parallelising them or consciously designing highly concurrent programs has been a goal in computing for some time now to fully utilise the available machine architectures and infrastructure. However, this attempt comes with a catch. In the 1960s, Gene Amdahl presented a theoretical upper bound\cite{amdahl} on the speedup of parallelised programs. It roughly states that even if a small amount of the program remains sequential (i.e. cannot be parallelised), the maximum achievable speedup will be rather limited. As an example, consider a program of which $75\%$ can be fully parallelised and denote this fraction with $p$, and let $s = 1 - p$ be the remaining sequential work. The theoretical speedup $S(n)$, where $n$ is the number of cores available, will than be no bigger than $S(n) \leq \lim_{n \to \infty} 1/(s + p/n) = 4$. With $p = 0.9$, the theoretical speedup bound is still only 10, even if we have an infinite number of processing cores available. In real-world applications, the implications of Amdahl's Law is not as daunting as it may first seem. Firstly, it assumes that the application operates on fixed problem size. However, today's massively concurrent machines allow computations on much larger datasets in the same amount of time. The argument that parallelisation is not just about speeding up programs but enabling them to deal with larger datasets is captured by Gustafson's Law\cite{gustafson}. Both Amdahl's Law and Gustafson's Law, however, fail to take communication into consideration. As systems get more and more complicated, so does the inherent complexity of communication between them; therefore, both laws paint a somewhat simplistic picture. Still, the main implication of both is that there are inherent limitations to parallelisation and that introducing $n$ more computing cores does not immediately result in $n$-fold speedup.

Another problem involving concurrent programming is achieving synchronisation. Synchronisation refers to how separate concurrent \textit{threads} manage and operate on shared data, i.e. ideally want maximum performance while ensuring that no concurrent operations overwrite each other or see inconsistent states. One of the more simple ways to achieve this is by using \textit{locks}. Consider for example the simple \textit{spinlock} with two operations: \texttt{lock()} and \texttt{unlock()}. Spinlocks (or locks in general) can be thought of as some global variable that threads acquire before executing operations on shared data. Those operations are said to reside in the \textit{critical section}. If one thread wants to operate on a shared datastructure, it must first acquire the lock, perform the operations and then release the lock. In the meanwhile, other threads wishing to perform their work on the shared datastructure \textit{spin}, until they can acquire the lock. One (of the many) problems with this simple way of achieving synchronisation, often referred to as \textit{coarse-grained locking} is that it is slow and does not scale well to highly concurrent applications. When the computation runs on many threads, even if they want to modify completely disjoint parts of a potentially large datastructure, they must wait in line to perform their actions. Therefore, computation is still sequential, even though many computing cores are available. A possible alternative is to use \textit{fine-grained locking} by associating certain parts of the datastructure with their own locks. That way, more concurrent threads can operate on different parts of the data. This can even be further enhanced and complicated by using \textit{readers-writers} locks, i.e. when each locked part of the datastructure is associated with two locks; one must be acquired for reading the object while the other must be acquired when writing the object. 

The above description about locks, by no means complete, is already quite complex and is still far from the whole picture. Mainly, it does not take into account how threads are scheduled by the operating system. Scheduling algorithms are \textit{non-deterministic}, which, when applied to algorithms, generally means that given the same input, the algorithm might produce different outputs. Therefore, we cannot make any assumptions on how a thread is scheduled or when it is preempted or descheduled. This creates a problem for locking: \textit{priority-inversion} happens when a lower-priority thread is preempted while holding onto a lock that a higher-priority thread needs. This means that the higher-priority thread needs to wait for the lower-priority one to be rescheduled and release the lock. Moreover, in datastructures where locks must be acquired one-after-another (also called \textit{hand-over-hand locking}) to reach a certain part of the structure, \textit{convoying} can occur, which means that if a thread further down the chain is delayed, all other threads behind it need to wait as well. \textit{Deadlock} occurs when threads try to acquire the same locks in a different order and execution grinds to a halt.

A possible way to move away from locks is the use of \textit{lock-free} primitives that are also detailed in Section \ref{subsection:sync}. Hardware operations like \textit{compare-and-swap} work by only allowing operations on shared data to succeed if the current value that a thread reads matches the actual value of the object (i.e. no other thread modified it in the meanwhile). These primitives, however, still have limitations and problems. Firstly, they usually operate on a single word in memory; therefore, only one location can be modified \textit{atomically} (i.e. inseparably). Moreover, by this limitation, more complex lock-free algorithms tend to have a very unnatural structure.

What might be a solution to the problems described above? On the one hand, locks are performant; however, there are inherent problems with them. Apart from priority-inversion, convoying, and deadlocks, generally speaking, programming with locks is hard as complex situations can arise that are difficult to debug. Lock-free primitives propose an alternative; however, more complex algorithms tend to have an unnatural structure to them due to the limitations of the primitives.

A potential solution is \textit{transactional memory}, a concept introduced by Herlihy and Moss in 1993\cite{herlihy-moss}. Transactions, which are already an established phenomenon in databases, are a set of instructions operating on some shared data which have the properties that they are \textit{serialisable} and \textit{atomic}. Serializability means that transactions appear to execute one after another and never seem to interleave. Atomicity indicates that transactions make speculative changes to memory which they only make atomically visible by \textit{committing} if no inconsistencies are found (i.e. no other transaction modified the shared data that the transaction accessed in the meanwhile). Suppose there is a conflict, the transaction aborts and can retry executing its operations. This provides a nice abstraction away from the "dirty details" of concurrent algorithms, as it is the transactional runtime system that takes care of the hard work. Wrapping instructions in transactions that modify a shared datastructure and retrying them until they are able to commit also provides a sufficiently straightforward interface, without the need to worry about locking, priority-inversion or unnaturally structured complex algorithms.

\section{Goals}
The goals of the paper are to present the history of transactional memory and, specifically, how it can be applied to concurrent datastructure design. Designing concurrent datastructures usually involve hand-crafting them to certain use cases, and the process involved is very demanding - often best left to experts. Using the transactional approach, however, the paper would like to show that designing transactional datastructures is not much different from designing sequential ones. In cases when the application performance isn't critical, the transactional interface can offer a very accessible approach to dealing with concurrency. The paper proposes two of such transactional datastructures that will be detailed in Section \ref{section:trb} and \ref{section:skip}. Finally, an experiment is described in Chapter \ref{chapter:eval} to investigate the performance of certain transactional memory implementations underlying the datastructures and answer the research questions outlined below.

\section{Research Questions}
In order to realise the goals of the paper, the following research questions are explored related to transactional datastructure design:

\begin{enumerate}
    \item How does the locking scheme of lock-based STM implementations affect the insertion performance of concurrent red-black trees and skiplists?
    \item How well suited is transactional programming for concurrent datastructures in terms of ease-of-design?
\end{enumerate}

Research Question 1 is explored thoroughly in Chapter \ref{chapter:eval}, where the proposed STM implementations are tested on two transactional datastructures: a Red-Black Tree and Skiplist.

Research Question 2 is a follow-up and a minor one, where based on the results described in Section \ref{section:res} further conclusions on the ease of transactional design can be drawn.

\section{Organisation}
The rest of the paper is organised as follows: Chapter 2 introduces common concepts and terminology regarding progress conditions and synchronisation primitives. Chapter 3 gives an overview of Transactional Memory both in hardware and software by presenting a handful of papers and their approaches. Chapter 4 details the implementations proposed by the paper, including two STM versions: one using encounter-order locking, the other using commit-time locking. Moreover, two transactional datastructures, a red-black tree and a skiplist, are described. Chapter 5 describes the evaluation of the implementations and presents and discusses the results. Finally, Chapter 6 gives the concluding remarks of the paper with possible future directions to explore.